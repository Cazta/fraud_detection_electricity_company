{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client data\n",
    "df_clients =pd.read_csv('data/client_train.csv')\n",
    "\n",
    "# invoice score\n",
    "df_invoice =pd.read_csv('data/invoice_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column descriptions:\n",
    "\n",
    "### Client Data\n",
    "\n",
    "* Client_id: Unique id for client\n",
    "* District: District where the client is\n",
    "* Client_catg: Category client belongs to\n",
    "* Region: Area where the client is\n",
    "* Creation_date: Date client joined\n",
    "* Target: fraud:1 , not fraud: 0\n",
    "\n",
    "### Invoice Data\n",
    "\n",
    "* Client_id: Unique id for the client\n",
    "* Invoice_date: Date of the invoice\n",
    "* Tarif_type: Type of tax\n",
    "* Counter_number:\n",
    "* Counter_statue: takes up to 5 values such as working fine, not working, on hold statue, ect\n",
    "* Counter_code:\n",
    "* Reading_remarque: notes that the STEG agent takes during his visit to the client (e.g: If the counter shows something wrong, the agent gives a bad score)\n",
    "* Counter_coefficient: An additional coefficient to be added when standard consumption is exceeded\n",
    "* Consommation_level_1: Consumption_level_1\n",
    "* Consommation_level_2: Consumption_level_2\n",
    "* Consommation_level_3: Consumption_level_3\n",
    "* Consommation_level_4: Consumption_level_4\n",
    "* Old_index: Old index\n",
    "* New_index: New index\n",
    "* Months_number: Month number\n",
    "* Counter_type: Type of counter\n",
    "\n",
    " the consumption level refers to the threshold of consumption to which a certain price is attributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = coffee_quality[\"quality_score\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaningin and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined =  pd.merge(df_clients, df_invoice, on=\"client_id\", how=\"left\")\n",
    "#df_clients.join(df_invoice, on='client_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_joined.client_id.nunique())\n",
    "df_joined.counter_number.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.counter_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.client_catg.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.tarif_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.groupby('counter_statue').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.groupby('counter_code').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.counter_code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.reading_remarque.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.groupby('reading_remarque').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.counter_coefficient.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.old_index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Dos\n",
    "\n",
    "* remove client_id column\n",
    "* turn into dummies: client_catg, tarif_type\n",
    "* change target to 0,1 - DONE\n",
    "* turn creation_date, invoice_date  into datetime or something else - DONE\n",
    "* feature engineering: number of counters per client. then drop client_id, counter_number\n",
    "* counter_statue: turn strings 0-5 into int, check percentage of values not 0-5, check for pattern, remove - DONE\n",
    "* counter_code: either dummies or find info on steg site or drop?\n",
    "* reading_remarque: clean? turn into dummies\n",
    "* counter_coefficient: try both?: treat numerically and as dummies?\n",
    "* rescale consommation_level1 ... _4\n",
    "* drop index old and new\n",
    "* rescale months_number\n",
    "* turn into dummy counter_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many duplicated rows exist in the data frame\n",
    "df_joined.duplicated().value_counts()\n",
    "\n",
    "# there were 11 duplicatge rows, drop duplicates\n",
    "df_joined.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined['counter_statue'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter_statue: turn strings 0-5 into int, check percentage of values not 0-5, check for pattern, remove\n",
    "\n",
    "df_joined['counter_statue'] = df_joined['counter_statue'].map({\n",
    "    '0': 0,\n",
    "    0: 0, \n",
    "    1: 1,\n",
    "    2 : 2,\n",
    "    3: 3,\n",
    "    4: 4,  \n",
    "    5: 5,\n",
    "    '5': 5,\n",
    "    '1': 1,\n",
    "    '4': 4, \n",
    "    'A': np.nan,\n",
    "    618: np.nan, \n",
    "    269375: np.nan,\n",
    "    46: np.nan, \n",
    "    420: np.nan,\n",
    "    769: np.nan, \n",
    "    })\n",
    "df_joined['counter_statue'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing distribution\n",
    "\n",
    "#msno.matrix(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fraction of data we would lose : 0.001%\n",
    "\n",
    "print(f\"numbers of rows : {df_joined.shape[0]}\")\n",
    "print(f\"missing values in counter statue : {round(df_joined.counter_statue.isna().sum()/df_joined.shape[0]*100,4)} %\")\n",
    "\n",
    "# copy df\n",
    "df_processed = df_joined.copy()\n",
    "# drop NaN\n",
    "df_processed.dropna(inplace=True, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change target from float to int (0,1)\n",
    "df_processed.target = df_processed.target.astype(int)\n",
    "df_processed.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn columns invoice date and creation date to datetime\n",
    "\n",
    "df_processed['invoice_date'] = pd.to_datetime(df_processed['invoice_date'], format='%Y-%m-%d')\n",
    "df_processed['creation_date'] = pd.to_datetime(df_processed['creation_date'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df_processed[['disrict', 'client_catg', 'region', 'creation_date',\n",
    "       'target', 'invoice_date', 'tarif_type', 'counter_number',\n",
    "       'counter_statue', 'counter_code', 'reading_remarque',\n",
    "       'counter_coefficient', 'consommation_level_1', 'consommation_level_2',\n",
    "       'consommation_level_3', 'consommation_level_4', 'old_index',\n",
    "       'new_index', 'months_number']].corr(), annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parplot took a lot of time so we did not see it so far and left it for now\n",
    "\n",
    "# pairplot\n",
    "\n",
    "#fig = plt.figure(figsize=(20,10))\n",
    "#sns.pairplot(df_processed[['disrict', 'client_catg', 'region', 'creation_date', 'invoice_date', 'tarif_type', 'counter_number',\n",
    "       'counter_statue', 'counter_code', 'reading_remarque',\n",
    "       'counter_coefficient', 'consommation_level_1', 'consommation_level_2',\n",
    "       'consommation_level_3', 'consommation_level_4', 'months_number', 'target']], hue='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our target has the highest correlation with the client category, we will take a closer look at that: \n",
    "df_processed.client_catg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of target for each client category individually, descending by clients in category:\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "sns.histplot(data= df_processed.query('client_catg == 11'), x = 'client_catg', hue='target', stat='percent');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "sns.histplot(data= df_processed.query('client_catg == 51'), x = 'client_catg', hue='target', stat='percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "sns.histplot(data= df_processed.query('client_catg == 12'), x = 'client_catg', hue='target', stat='percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the highest amount of fraud happens by clients assigned to category 51, so our very fist guess for a hypothesis and baseline mode ist:\n",
    "\n",
    "# client category is the best predictor for fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "* run baseline model and print the different scores for it\n",
    "* Drop old and new index\n",
    "* Train-Test-Split\n",
    "* Define Target y, and Features X\n",
    "* Feature Engineering\n",
    "* Dummy Creating\n",
    "* Rescaling based on train, apply to test\n",
    "* dropping other unused columns\n",
    "* export X-test, X_train, y_test and y_train to a new notebook for modeling (find library to help us with that)\n",
    "* individual modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping Quakers column and unnamed\n",
    "#changing one of the altitude to log and droping the original\n",
    "X_train[\"altitude_mean_log\"] = np.log(X_train[\"altitude_mean_meters\"])\n",
    "X_train.drop(['altitude_mean_meters'], axis=1, inplace=True)\n",
    "X_train.drop(['Quakers'], axis=1, inplace=True)\n",
    "X_train.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altitude_low_meters_mean = X_train[\"altitude_low_meters\"].mean()\n",
    "altitude_high_meters_mean = X_train[\"altitude_high_meters\"].mean()\n",
    "altitude_mean_log_mean = X_train[\"altitude_mean_log\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with mean.. \n",
    "X_train[\"altitude_low_meters\"] = X_train[\"altitude_low_meters\"].fillna(altitude_low_meters_mean)\n",
    "X_train[\"altitude_high_meters\"] = X_train[\"altitude_high_meters\"].fillna(altitude_high_meters_mean)\n",
    "X_train[\"altitude_mean_log\"] = X_train[\"altitude_mean_log\"].fillna(altitude_mean_log_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"altitude low meters mean is {altitude_low_meters_mean}\")\n",
    "print(f\"altitude_high_meters_mean is {altitude_high_meters_mean}\")\n",
    "print(f\"altitude_mean_log_mean is {altitude_mean_log_mean}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in order to exemplify how the predict will work.. we will save the y_train\n",
    "X_test.to_csv(\"data/X_test.csv\")\n",
    "y_test.to_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_train_pred = reg.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping Quakers column and unnamed\n",
    "#changing one of the altitude to log and droping the original\n",
    "X_test[\"altitude_mean_log\"] = np.log(X_test[\"altitude_mean_meters\"])\n",
    "X_test.drop(['altitude_mean_meters'], axis=1, inplace=True)\n",
    "X_test.drop(['Quakers'], axis=1, inplace=True)\n",
    "X_test.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "# fillna with mean.. \n",
    "X_test[\"altitude_low_meters\"] = X_test[\"altitude_low_meters\"].fillna(altitude_low_meters_mean)\n",
    "X_test[\"altitude_high_meters\"] = X_test[\"altitude_high_meters\"].fillna(altitude_high_meters_mean)\n",
    "X_test[\"altitude_mean_log\"] = X_test[\"altitude_mean_log\"].fillna(altitude_mean_log_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
