{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues (task) covered in this feature notebook :\n",
    "\n",
    "*  explore data a bit w/modules, functions and plots\n",
    "*  baseline model test\n",
    "    - Logistic regression or Dummy Classifier\n",
    "    - or Clustering algorithm\n",
    "    - or treebased\n",
    "* base metrics \n",
    "    - precision_recall\n",
    "    - f1_score\n",
    "    - classification report\n",
    "    - confusion matrix\n",
    "    - area under the curve\n",
    "\n",
    "* train_test_split\n",
    "\n",
    "\n",
    "[Project Issue Link](https://github.com/users/Cazta/projects/1/views/2?pane=issue&itemId=32899160)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "# for reproducibility sake\n",
    "RSEED = 42\n",
    "\n",
    "# data path to load version 1 of the processed fraud data\n",
    "data_path = 'data/fraud_data_processed_V1.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, get information on dataset and display descriptive summary\n",
    "\n",
    "\n",
    "\n",
    "def get_data_summary(data_path = None, data= None, desc_sm = False, no_unq = False, *args, **kwargs): \n",
    "\n",
    "    if (data is None) and (data_path is None):\n",
    "        raise ValueError('''Either enther a data path or a dataset (dataframe)\n",
    "                    \n",
    "                        'data' : a dataset (dataframe)\n",
    "                        'datapath' : a data path used to load a csv data file\n",
    "                    \n",
    "                        ''') \n",
    "\n",
    "    elif (data is None) and (data_path is not None):\n",
    "        data = pd.read_csv( data_path) \n",
    "    else:\n",
    "        data = data\n",
    "\n",
    "\n",
    "    print (f\"Dataset shape: {data.shape}\") \n",
    "\n",
    "    print('_____'*10)\n",
    "\n",
    "    print(f''' \n",
    "    Number of observations : {data.shape[0]}\n",
    "    Number of features : {data.shape[1]}\n",
    "        ''')\n",
    "\n",
    "    print('_____'*10)\n",
    "\n",
    "    print (\"Dataset sample: \") \n",
    "    print('_____'*10)\n",
    "\n",
    "    display(data.head())\n",
    "\n",
    "    print('_____'*10)\n",
    "\n",
    "    if desc_sm:\n",
    "        print (\"Dataset descriptive summary: \") \n",
    "        print('_____'*10)\n",
    "\n",
    "        display(data.describe().T.style.format('{:.2f}'))\n",
    "\n",
    "    print('_____'*10)\n",
    "\n",
    "    if no_unq:\n",
    "        print (\"Unique values/classes for dataset features: \") \n",
    "        print('_____'*10)\n",
    "\n",
    "        display(data.nunique())\n",
    "\n",
    "\n",
    "    return data \n",
    "\n",
    "# load version 1 of the processed fraud data\n",
    "\n",
    "df = get_data_summary(data_path = data_path, desc_sm = True, no_unq = True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Brief Exploration\n",
    "\n",
    "* Stakeholder : ZINDI, STEG (Tunisian Company of Electricity and Gas)\n",
    "* Business objective : To use client's billing history for detecting which clients are fraudulently manipulating their energy (electricity and gas) meters\n",
    "* open questions : \n",
    "    - What does the consumption  (consumation?) levels tell us about fraudelent activities? \n",
    "    - What does the recorded reading remark  tell us about fraudelent activities? \n",
    "    - etc\n",
    "\n",
    "* hypothesis statemnets :\n",
    "\n",
    "\n",
    "* aggregating some data to gain insight\n",
    "* basic plots to investigate the data visually\n",
    "    - countplots\n",
    "    - \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by client id's using the aggregate : count\n",
    "\n",
    "df_clients = df.groupby('client_id').count().reset_index()\n",
    "\n",
    "# curious grouped dataframe summary : by client's id count\n",
    "\n",
    "df_clients_sm = get_data_summary(data = df_clients)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# countplots of specific features by target\n",
    "\n",
    "cat_features = ['disrict', 'client_catg', 'region',  'tarif_type', \n",
    "                    'counter_statue', 'reading_remarque', 'counter_coefficient']\n",
    "\n",
    "for i in cat_features:\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    sns.countplot(data=df, x=df[i], hue=df[\"target\"])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouby reading remark for dataset with specific features\n",
    "\n",
    "feat_1 =  ['target', 'counter_number', 'counter_statue', 'counter_code', \n",
    "        'counter_coefficient', 'consommation_level_1', 'consommation_level_2',\n",
    "        'consommation_level_3', 'consommation_level_4', 'old_index',\n",
    "        'new_index']\n",
    "\n",
    "\n",
    "read_mark = df.groupby(\"reading_remarque\")[feat_1].count().reset_index()\n",
    "read_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_2 =  ['reading_remarque', 'counter_number', 'counter_statue', 'counter_code', \n",
    "        'counter_coefficient', 'consommation_level_1', 'consommation_level_2',\n",
    "        'consommation_level_3', 'consommation_level_4', 'old_index',\n",
    "        'new_index']\n",
    "\n",
    "\n",
    "target_count = df.groupby(\"target\")[feat_2].count().reset_index()\n",
    "target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumation levels, counter details and target\n",
    "\n",
    "cons_count =  ['target', 'counter_number',\n",
    "        'counter_statue', 'counter_code', 'reading_remarque',\n",
    "        'counter_coefficient', 'consommation_level_1', 'consommation_level_2',\n",
    "        'consommation_level_3', 'consommation_level_4', 'old_index',\n",
    "        'new_index']\n",
    "\n",
    "# date related feature variables\n",
    "\n",
    "date_features = ['target', 'creation_date',  'invoice_date',  'months_number']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modelling\n",
    "\n",
    "* state baseline features\n",
    "* define x (input data) and y (target)\n",
    "* train_test_split \n",
    "* run baseline model : logistic reg?  sgd? knn? \n",
    "\n",
    "\n",
    "# Evaluate the baseline model\n",
    "\n",
    "* get evaluation scores using features from the initial data\n",
    "    - Area under the curve score (roc_auc_score)\n",
    "    - precision score\n",
    "    - recall score\n",
    "    - accuracy score\n",
    "    - balanced accuracy score\n",
    "    - confusion matrix\n",
    "    - classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature variables for version 1 (df) of processed fraud data for baseline model \n",
    "\n",
    "base_features = ['disrict', 'client_catg', 'region',  \n",
    "                'tarif_type', 'counter_number','counter_statue', \n",
    "                'counter_code', 'reading_remarque','counter_coefficient', \n",
    "                'consommation_level_1', 'consommation_level_2', 'consommation_level_3', \n",
    "                'consommation_level_4', 'old_index', 'new_index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the base x (input data) features and y (target) feature\n",
    "\n",
    "x_base = df[base_features]\n",
    "y = df[\"target\"]\n",
    "\n",
    "\n",
    "print(f\"shape of baseline input data: {x_base.shape}\")\n",
    "print(f\"shape of target data: {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split for base\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_base, y, test_size=0.25, random_state=RSEED)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "y_pred_lr = log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics : confusion matrix,, accuracy, balance accuracy, classification report\n",
    "\n",
    "def eval_metrics(y_test, y_pred): \n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        Function to calculate the accuracy and balanced accuracy score for imbalanced data, get the confusion \n",
    "        matrix as well as the classification report of the ML \n",
    "        model based on the predictions and true target values for the test set.\n",
    "\n",
    "    Args:\n",
    "        y_test (numpy.ndarray): test target data\n",
    "        y_pred (numpy.ndarray): predictions based on test data\n",
    "    \"\"\"    \n",
    "    \n",
    "    print(\"-----\"*15)\n",
    "    print(f'''Confusion Matrix: \n",
    "    {confusion_matrix(y_test, y_pred)} ''') \n",
    "    \n",
    "    print(\"-----\"*15)\n",
    "    print (f''' Accuracy : \n",
    "    {(accuracy_score(y_test, y_pred).round(2)) * 100} ''')\n",
    "\n",
    "    print(\"-----\"*15)\n",
    "    print (f''' Balanced Accuracy : \n",
    "    {(balanced_accuracy_score(y_test, y_pred).round(2)) * 100} ''')\n",
    "    \n",
    "    print(\"-----\"*15)\n",
    "    print(f'''Report :  \n",
    "    {classification_report(y_test, y_pred)} ''') \n",
    "\n",
    "\n",
    "\n",
    "eval_metrics(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval scoring metrics : recall, precisoon, f1_score, roc_auc_score, fpr, tpr\n",
    "\n",
    "\n",
    "def evaluate_model(predictions, probs, train_predictions, train_probs):\n",
    "    \"\"\"Compare machine learning model to baseline performance.\n",
    "    Computes statistics and shows ROC curve.\"\"\"\n",
    "    \n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(y_test, [1 for _ in range(len(y_test))])\n",
    "    baseline['precision'] = precision_score(y_test, [1 for _ in range(len(y_test))])\n",
    "    baseline['f1_score'] = f1_score(y_test, [1 for _ in range(len(y_test))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['recall'] = recall_score(y_test, predictions)\n",
    "    results['precision'] = precision_score(y_test, predictions)\n",
    "    results['f1_score'] = f1_score(y_test, predictions)\n",
    "    results['roc'] = roc_auc_score(y_test, probs)\n",
    "    \n",
    "    # train_results = {}\n",
    "    # train_results['recall'] = recall_score(y_test, train_predictions)\n",
    "    # train_results['precision'] = precision_score(y_test, train_predictions)\n",
    "    # train_results['f1_score'] = f1_score(y_test, predictions)\n",
    "    # train_results['roc'] = roc_auc_score(y_test, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'f1_score', 'roc']:\n",
    "        #print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} ')\n",
    "\n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(y_test, probs)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model \n",
    "\n",
    "train_probs_lr = log_reg.predict_proba(x_train)[:, 1]\n",
    "test_probs_lr = log_reg.predict_proba(x_test)[:, 1]\n",
    "\n",
    "train_preds_lr = log_reg.predict(x_train)\n",
    "test_preds_lr = log_reg.predict(x_test)\n",
    "\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_probs_lr)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(y_test, test_probs_lr)}')\n",
    "print(f'Baseline ROC AUC: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --!\n",
    "evaluate_model(test_preds_lr, test_probs_lr, train_preds_lr, train_probs_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-neighbours classifier as baseline?\n",
    "\n",
    "\n",
    "# initialize and fit/train model on data\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "# predict on test\n",
    "\n",
    "y_pred_knn = knn.predict(x_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make probability predictions\n",
    "train_probs_knn = knn.predict_proba(x_train)[:, 1]\n",
    "test_probs_knn = knn.predict_proba(x_test)[:, 1]\n",
    "\n",
    "train_preds_knn = knn.predict(x_train)\n",
    "test_preds_knn = knn.predict(x_test)\n",
    "\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_probs_knn)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(y_test, test_probs_knn)}')\n",
    "print(f'Baseline ROC AUC: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(y_test, y_pred_knn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_model(test_preds_knn, test_probs_knn, train_preds_knn, train_probs_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgdclassifier as baseline?\n",
    "\n",
    "# Fit and evaluate model without hyperparameter tuning using cross validation and unscaled data \n",
    "sgd_classifier = SGDClassifier(random_state=RSEED)\n",
    "scores = cross_val_score(sgd_classifier, x_train, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "# Evaluation \n",
    "print('Score (unscaled):', round(scores.mean(), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save current version of processed data for use later\n",
    "\n",
    "# df_processed.to_csv('data/fraud_data_processed_V1.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
