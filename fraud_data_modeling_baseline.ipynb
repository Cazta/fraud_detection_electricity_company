{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues (task) covered in this feature notebook :\n",
    "\n",
    "*  explore data a bit w/modules, functions and plots\n",
    "*  baseline model test\n",
    "    - KNN classifier \n",
    "    - Logistic regression or Dummy Classifier ?\n",
    "    - or Clustering algorithm ?\n",
    "    - or treebased ?\n",
    "* base metrics \n",
    "    - precision, recall\n",
    "    - f1_score\n",
    "    - accuracy, balanced accuracy\n",
    "    - classification report\n",
    "    - confusion matrix\n",
    "    - area under the curve\n",
    "\n",
    "* train_test_split\n",
    "\n",
    "\n",
    "[Project Issue Link](https://github.com/users/Cazta/projects/1/views/2?pane=issue&itemId=32899160)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# import custom functions\n",
    "\n",
    "from custom_functions import get_data_summary, our_metrics, eval_metrics, evaluate_model\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "# for reproducibility sake\n",
    "RSEED = 42\n",
    "\n",
    "# data path to load version 1 of the processed fraud data\n",
    "data_path = 'data/fraud_data_processed_V1.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stored DataFrame\n",
    "%store -r df_processed_dmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import list of numerical features\n",
    "%store -r num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this contains all the numerical (unscaled) and dummies from categorical\n",
    "df_cleaned = df_processed_dmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load version 1 of the processed fraud data\n",
    "\n",
    "#df = get_data_summary(data_path = data_path, desc_sm = True, no_unq = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grouping by client id's using the aggregate : count\n",
    "\n",
    "# df_gb_clients = df.groupby('client_id').count().reset_index()\n",
    "\n",
    "# # curious grouped dataframe summary : by client's id count\n",
    "\n",
    "# #how many invoices per client\n",
    "# df_gb_clients_sm = get_data_summary(data = df_gb_clients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Brief Exploration / Hypothesis\n",
    "\n",
    "* Stakeholder : STEG (Tunisian Company of Electricity and Gas)\n",
    "* Business objective : To use client's billing history to predict which clients are probably fraudulently manipulating their energy (electricity and gas) meters\n",
    "* open questions : \n",
    "    - What does the consumption  (consummation?) levels tell us about fraudulent activities? \n",
    "    - What does the recorded reading remark  tell us about fraudulent activities? \n",
    "    - etc\n",
    "\n",
    "* hypothesis statements :\n",
    "    - very low consumption levels are an indicator of fraud.\n",
    "    - client_category is a predictor for fraud.\n",
    "    - geographical location (district, region) is an indicator for fraud.\n",
    "    - not having any consummation at level 2 and higher is an indicator for fraud.\n",
    "    - certain reading_remarques are an indicator of fraud.\n",
    "\n",
    "* possible future stakeholders / use cases: \n",
    "    - other Gas or Electricity Companies\n",
    "    - Law enforcement agencies\n",
    "    - state authorities for utilities\n",
    "\n",
    "\n",
    "* aggregating some data to gain insight\n",
    "* basic plots to investigate the data visually\n",
    "    - countplots\n",
    "    - \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by client id's using the aggregate : count\n",
    "\n",
    "# df_gb_clients = df.groupby('client_id').count().reset_index()\n",
    "\n",
    "# # curious grouped dataframe summary : by client's id count\n",
    "\n",
    "# #how many invoices per client\n",
    "# df_gb_clients_sm = get_data_summary(data = df_gb_clients)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # countplots of specific features by target\n",
    "\n",
    "# cat_features = ['disrict', 'client_catg', 'region',  'tarif_type', \n",
    "#                     'counter_statue', 'reading_remarque', 'counter_coefficient']\n",
    "\n",
    "# for i in cat_features:\n",
    "#     fig = plt.figure(figsize=(12,5))\n",
    "#     sns.countplot(data=df, x=df[i], hue=df[\"target\"])\n",
    "#     plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(12,5))\n",
    "#sns.countplot(df.query('tarif_type >= 12 and tarif_type <=14'), x=df.query('tarif_type >= 12 and tarif_type <=14')['tarif_type'], hue=df[\"target\"])\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12,5))\n",
    "# sns.countplot(df, x=df['target'])\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12,5))\n",
    "# sns.countplot(df.query('consommation_level_2 == 0 and consommation_level_3 == 0 and consommation_level_4 == 0'), x=df.query('consommation_level_2 == 0 and consommation_level_3 == 0 and consommation_level_4 == 0')['target'])\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12,5))\n",
    "# sns.countplot(df, x=df.query('consommation_level_4 != 0')['target'])\n",
    "# plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks on hypotheses\n",
    "* District is __NOT__ an indicator for fraud.\n",
    "* Most frauds are committed by clients of category 11. However, there are not many clients of the other two categories.\n",
    "* the top 5 regions with a high number of frauds being committed are: 101, 103, 104, 107, 311. Still, most customers are not committing fraud in those regions. Regions cannot be used as a single indicator for fraud.\n",
    "* geographical locations are not a sole indicator of fraud\n",
    "* tarif_type is not a good indicator for fraud\n",
    "* consommation_level: The fraud percentage in level 4 is higher than for level 1 only. the hypothesis is declined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grouby reading remark for dataset with specific features\n",
    "\n",
    "# feat_1 =  ['target', 'counter_number', 'counter_statue', 'counter_code', \n",
    "#         'counter_coefficient', 'consommation_level_1', 'consommation_level_2',\n",
    "#         'consommation_level_3', 'consommation_level_4', 'old_index',\n",
    "#         'new_index']\n",
    "\n",
    "\n",
    "# read_mark = df.groupby(\"reading_remarque\")[feat_1].count().reset_index()\n",
    "# read_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_2 =  ['reading_remarque', 'counter_number', 'counter_statue', 'counter_code', \n",
    "#         'counter_coefficient', 'consommation_level_1', 'consommation_level_2',\n",
    "#         'consommation_level_3', 'consommation_level_4', 'old_index',\n",
    "#         'new_index']\n",
    "\n",
    "\n",
    "# target_count = df.groupby(\"target\")[feat_2].count().reset_index()\n",
    "# target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # consumation levels, counter details and target\n",
    "\n",
    "# cons_count =  ['target', 'counter_number',\n",
    "#         'counter_statue', 'counter_code', 'reading_remarque',\n",
    "#         'counter_coefficient', 'consommation_level_1', 'consommation_level_2',\n",
    "#         'consommation_level_3', 'consommation_level_4', 'old_index',\n",
    "#         'new_index']\n",
    "\n",
    "# # date related feature variables\n",
    "\n",
    "# date_features = ['target', 'creation_date',  'invoice_date',  'months_number']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modelling\n",
    "\n",
    "* state baseline features\n",
    "* define x (input data) and y (target)\n",
    "* train_test_split \n",
    "* run baseline model : KNN as baseline\n",
    "  - logistic reg?  sgd? \n",
    "\n",
    "\n",
    "# Evaluate the baseline model\n",
    "\n",
    "* get evaluation scores using features from the initial data\n",
    "    - Area under the curve score (roc_auc_score)\n",
    "    - precision score\n",
    "    - recall score\n",
    "    - accuracy score\n",
    "    - balanced accuracy score\n",
    "    - confusion matrix\n",
    "    - classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Predicting all to be non-fraud\n",
    "\n",
    "* Out baseline model will be a simply to predict that no one commits fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics\n",
    "* we chose our model metric to be the f_beta-score or the f1-score, because the target is unbalanced we can not use accuracy\n",
    "* we need to detect true positives, but limit the number of false positives to a minimum\n",
    "* our chosen evaluation metric therefore is the f_beta-score with a value of beta that we will determine during the modeling process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature variables of our data\n",
    "\n",
    "features = df_cleaned.columns.to_list()\n",
    "features.remove('target')\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the base X (input data) features and y (target) feature\n",
    "\n",
    "X = df_cleaned[features]\n",
    "y = df_cleaned[\"target\"]\n",
    "\n",
    "\n",
    "print(f\"shape of baseline input data: {X.shape}\")\n",
    "print(f\"shape of target data: {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split for base\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RSEED, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Predict all non-fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining baseline model that predicts no one commits fraud\n",
    "def baseline_model(df):\n",
    "    y_pred = [0 for x in df.index]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions with baseline model for test set\n",
    "y_baseline_test = baseline_model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_metrics(y_test, y_baseline_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: <= 15% consumption is fraud\n",
    "\n",
    "predicting fraud for clients with consummation in the lowest 15 % (change_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_idx = df_cleaned[['index_change']].quantile(q=0.15)[0]\n",
    "quantile_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.query('index_change <= @quantile_idx')['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining baseline model that predicts no one commits fraud\n",
    "def baseline_model_idx(df, quantile_idx):\n",
    "    y_pred = [1 if x <= quantile_idx else 0 for x in df['index_change']]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions with baseline model for test set\n",
    "y_baseline_idx_test = baseline_model_idx(X_test, quantile_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_metrics(y_test, y_baseline_idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable holding numerical features for rescaling\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with Minmax-scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_train_minmax_scaled = minmax_scaler.fit_transform(X_train[num_features])\n",
    "X_test_minmax_scaled = minmax_scaler.transform(X_test[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating minmax_scaled to dummys\n",
    "\n",
    "X_train_minmax = np.concatenate([X_train_minmax_scaled, X_train.drop(num_features, axis=1)], axis=1)\n",
    "X_test_minmax = np.concatenate([X_test_minmax_scaled, X_test.drop(num_features, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescaling: Standardization-Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "X_train_std_scaled = std_scaler.fit_transform(X_train[num_features])\n",
    "X_test_std_scaled = std_scaler.transform(X_test[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating std_scaled to dummys\n",
    "\n",
    "X_train_std = np.concatenate([X_train_std_scaled, X_train.drop(num_features, axis=1)], axis=1)\n",
    "X_test_std = np.concatenate([X_test_std_scaled, X_test.drop(num_features, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-Model Assignment\n",
    "\n",
    "* logistic regression = Anas\n",
    "* Decision Tree = \n",
    "* Extra Tree \n",
    "* Random forest = Anna\n",
    "* KNN =  Grace \n",
    "* Easy Ensemble classifier = Lana\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store variables for ML-Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the preprocessed, cleaned and scaled features and target\n",
    "# %store X_train_minmax\n",
    "# %store X_test_minmax \n",
    "# %store X_train_std\n",
    "# %store X_test_std\n",
    "# %store y_test\n",
    "# %store y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing to csv as storemagic keeps crashing the kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing to csv as %store keeps crashing the Kernel\n",
    "\n",
    "pd.DataFrame(X_train_minmax).to_csv('data/X_train_minmax.csv', index=False)\n",
    "pd.DataFrame(X_test_minmax).to_csv('data/X_test_minmax.csv', index=False)\n",
    "pd.DataFrame(X_train_std).to_csv('data/X_train_std.csv', index=False)\n",
    "pd.DataFrame(X_test_std).to_csv('data/X_test_std.csv', index=False)\n",
    "pd.DataFrame(y_test).to_csv('data/y_test.csv', index=False)\n",
    "pd.DataFrame(y_train).to_csv('data/y_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg = LogisticRegression()\n",
    "# log_reg.fit(X=_train, y_train)\n",
    "\n",
    "# y_pred_lr = log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model \n",
    "\n",
    "train_probs_lr = log_reg.predict_proba(x_train)[:, 1]\n",
    "test_probs_lr = log_reg.predict_proba(x_test)[:, 1]\n",
    "\n",
    "train_preds_lr = log_reg.predict(x_train)\n",
    "test_preds_lr = log_reg.predict(x_test)\n",
    "\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_probs_lr)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(y_test, test_probs_lr)}')\n",
    "print(f'Baseline ROC AUC: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --!\n",
    "evaluate_model(test_preds_lr, test_probs_lr, train_preds_lr, train_probs_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-neighbours classifier as baseline?\n",
    "\n",
    "\n",
    "# initialize and fit/train model on data\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "# predict on test\n",
    "\n",
    "y_pred_knn = knn.predict(x_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make probability predictions\n",
    "train_probs_knn = knn.predict_proba(x_train)[:, 1]\n",
    "test_probs_knn = knn.predict_proba(x_test)[:, 1]\n",
    "\n",
    "train_preds_knn = knn.predict(x_train)\n",
    "test_preds_knn = knn.predict(x_test)\n",
    "\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_probs_knn)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(y_test, test_probs_knn)}')\n",
    "print(f'Baseline ROC AUC: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(y_test, y_pred_knn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_model(test_preds_knn, test_probs_knn, train_preds_knn, train_probs_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgdclassifier \n",
    "\n",
    "# Fit and evaluate model without hyperparameter tuning using cross validation and unscaled data \n",
    "sgd_classifier = SGDClassifier(random_state=RSEED)\n",
    "scores = cross_val_score(sgd_classifier, x_train, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "# Evaluation \n",
    "print('Score (unscaled):', round(scores.mean(), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save current version of processed data for use later\n",
    "\n",
    "# df_processed.to_csv('data/fraud_data_processed_V1.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
