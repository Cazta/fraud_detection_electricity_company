{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "#imbalanced learning\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "# import custom functions\n",
    "from custom_functions import get_data_summary, our_metrics, eval_metrics, evaluate_model\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility sake\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make scorer Fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the Fbeta scorers needed for the grid search\n",
    "f15_scorer = make_scorer(fbeta_score, beta=1.5)\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "fthree_scorer = make_scorer(fbeta_score, beta=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import test and train data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't use storemagic for lare Dataframes /Arrays, it will keep crashing!\n",
    "# # %store shows all stored variables. \n",
    "# # %store -z deletes all stored variables from memory\n",
    "# # retrieve the preprocessed, cleaned and scaled features and target\n",
    "# %store -r X_train_minmax\n",
    "# %store -r X_test_minmax \n",
    "# %store -r X_train_std\n",
    "# %store -r X_test_std\n",
    "# %store -r y_test\n",
    "# %store -r y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tree = pd.read_csv('data/X_train_tree.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tree = pd.read_csv('data/X_test_tree.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tree = pd.read_csv('data/y_test_tree.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tree = pd.read_csv('data/y_train_tree.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tree = np.ravel(y_test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tree = np.ravel(y_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EasyEnsembleClassifier\n",
    "\n",
    "### What is the EasyEnsembleClassifier?\n",
    "\n",
    "* Machine Learning Model for imbalanced data\n",
    "* Bag of balanced boosted learners.\n",
    "* Ensemble of AdaBoost learners trained on different balanced bootstrap samples. The balancing is achieved by random under-sampling.\n",
    "\n",
    "### AdaBoostClassifier\n",
    "\n",
    "[scikit-learn documentation](https://scikit-learn.org/stable/modules/ensemble.html#adaboost)\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights , , …,  to each of the training samples. Initially, those weights are all set to , so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence.\n",
    "\n",
    " By default, weak learners are decision stumps. Different weak learners can be specified through the estimator parameter. The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples to consider a split min_samples_split).\n",
    "\n",
    "The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initiate EasyEnsembleClassifier\n",
    "# eec = EasyEnsembleClassifier(random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit eec to train data\n",
    "# eec.fit(X_train_tree, y_train_tree)\n",
    "# # make prediction on test data\n",
    "# y_pred_eec = eec.predict(X_test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get performance metrics\n",
    "# our_metrics(y_test_tree, y_pred_eec)\n",
    "# # print confusion matrix\n",
    "# # print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eec_rs = EasyEnsembleClassifier(random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': 'deprecated',\n",
       " 'estimator': None,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': None,\n",
       " 'random_state': 42,\n",
       " 'replacement': False,\n",
       " 'sampling_strategy': 'auto',\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eec_rs.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of the EasyEnsembleClassifier\n",
    "\n",
    "* 'base_estimator': 'deprecated', now 'estimator'\n",
    "* 'estimator': None, default=AdaBoostClassifier()\n",
    "* 'n_estimators': default=10, Number of AdaBoost learners in the ensemble.\n",
    "* 'n_jobs': default=None, Number of CPU cores used during the cross-validation loop. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n",
    "* 'random_state': default=None, If None, the random number generator is the RandomState instance used by np.random.\n",
    "* 'replacement': default=False, Whether or not to sample randomly with replacement or not.\n",
    "* 'sampling_strategy': default='auto'equivalent to 'not minority', Sampling information to sample the data set. float, str, dict, callable, default=’auto’. When float, it corresponds to the desired ratio of the number of samples in the minority class over the number of samples in the majority class after resampling. Therefore, the ratio is expressed as $N_m/N_{rM}$,\n",
    " where $N_m$ is the number of samples in the minority class and $N_{rM}$ is the number of samples in the majority class after resampling.\n",
    "* 'verbose': default=0, Controls the verbosity of the building process.\n",
    "* 'warm_start': default=False. When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "param_grid = { \"sampling_strategy\" : ['auto', 0.5, 0.8, 1, 1.2, 1.5],\n",
    "            \"replacement\" : [False, True],\n",
    "                \"n_estimators\" : [6, 8, 10, 12, 14]}\n",
    "#               \"C\" : loguniform(1e-5, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Random search\n",
    "Random_search_f2 = RandomizedSearchCV(eec_rs, param_grid, n_iter=500, scoring=ftwo_scorer, n_jobs=1, cv=cv, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute Random search\n",
    "Random_search_f2.fit(X_train_tree, y_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_f2 = Random_search_f2.predict(X_train_tree)\n",
    "y_pred_RS_f2 = Random_search_f2.predict(X_test_tree)\n",
    "\n",
    "print(\"Tuned hyperparameters :(best parameters) \",Random_search_f2.best_params_)\n",
    "print(\"Decision Metrics:\")\n",
    "our_metrics(y_test_tree, y_pred_RS_f2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
